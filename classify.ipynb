{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb5116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wzhzhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[WARNING] From c:\\Users\\wzhzhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[INFO] PyTorch version 2.5.1+cu121 available.\n",
      "[INFO] TensorFlow version 2.19.0 available.\n"
     ]
    }
   ],
   "source": [
    "from parser.models.question import (\n",
    "    Question,\n",
    "    SubQuestion,\n",
    "    SubSubQuestion,\n",
    "    MultipleChoiceQuestion,\n",
    ")\n",
    "from parser.models.syllabus import Sy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from typing import List, Optional\n",
    "import re\n",
    "import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd47a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        syllabuses: List[Syllabus],\n",
    "        batch_size: int = 64,\n",
    "        cache_path: str = \"syllabus_embeddings.pkl\",\n",
    "        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",  # Smaller general-purpose model\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.cache_path = cache_path\n",
    "        self.syllabus_objects = syllabuses\n",
    "\n",
    "        print(f\"Initializing SentenceTransformer with model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "        # Try to load cache, otherwise process syllabuses\n",
    "        if os.path.exists(cache_path) and self._try_load_cache():\n",
    "            print(f\"Loaded embeddings from cache: {cache_path}\")\n",
    "        else:\n",
    "            print(\"Processing syllabus content...\")\n",
    "            self._preprocess_syllabuses(syllabuses)\n",
    "            self._save_cache()\n",
    "\n",
    "    def _preprocess_syllabuses(self, syllabuses: List[Syllabus]) -> None:\n",
    "        # Create corpus and mapping\n",
    "        self.corpus = []\n",
    "        self.syllabus_mapping = []\n",
    "\n",
    "        for idx, syllabus in enumerate(syllabuses):\n",
    "            for point in syllabus.content:\n",
    "                text = point.lower().strip()\n",
    "                self.corpus.append(text)\n",
    "                self.syllabus_mapping.append(idx)\n",
    "\n",
    "        print(f\"Encoding {len(self.corpus)} syllabus points\")\n",
    "        self.corpus_embeddings = self.model.encode(\n",
    "            self.corpus,\n",
    "            batch_size=self.batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "        )\n",
    "\n",
    "    def _save_cache(self) -> None:\n",
    "        cache_data = {\n",
    "            \"corpus\": self.corpus,\n",
    "            \"corpus_embeddings\": self.corpus_embeddings,\n",
    "            \"syllabus_mapping\": self.syllabus_mapping,\n",
    "        }\n",
    "        with open(self.cache_path, \"wb\") as f:\n",
    "            pickle.dump(cache_data, f)\n",
    "        print(f\"Embeddings cached to {self.cache_path}\")\n",
    "\n",
    "    def _try_load_cache(self) -> bool:\n",
    "        try:\n",
    "            with open(self.cache_path, \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "                self.corpus = cache_data[\"corpus\"]\n",
    "                self.corpus_embeddings = cache_data[\"corpus_embeddings\"]\n",
    "                self.syllabus_mapping = cache_data[\"syllabus_mapping\"]\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Cache load failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def classify_all(\n",
    "        self, questions: List[Question | MultipleChoiceQuestion]\n",
    "    ) -> List[Question | MultipleChoiceQuestion]:\n",
    "        for question in tqdm.tqdm(questions, desc=\"Classifying questions\"):\n",
    "            self.classify(question)\n",
    "        return questions\n",
    "\n",
    "    def classify(\n",
    "        self, question: MultipleChoiceQuestion | Question | SubQuestion | SubSubQuestion\n",
    "    ) -> None:\n",
    "        if isinstance(question, MultipleChoiceQuestion):\n",
    "            question.syllabus = self.get_best_syllabus(question.text)\n",
    "            return\n",
    "        elif isinstance(question, SubQuestion):\n",
    "            if question.subsubquestions:\n",
    "                for subsubquestion in question.subsubquestions:\n",
    "                    self.classify(subsubquestion)\n",
    "        elif isinstance(question, Question):\n",
    "            if question.subquestions:\n",
    "                for subquestion in question.subquestions:\n",
    "                    self.classify(subquestion)\n",
    "\n",
    "        # Combine question text and answer for better matching\n",
    "        question_text = (\n",
    "            question.text + \" \" + (question.answer if question.answer else \"\")\n",
    "        )\n",
    "        question.syllabus = self.get_best_syllabus(question_text)\n",
    "\n",
    "    def get_best_syllabus(self, question_sentence: str, threshold = 0.5) -> Syllabus:\n",
    "        # Clean text: remove more than three connected dots and score markers\n",
    "        question_sentence = re.sub(r\"\\.{3,}\", \"\", question_sentence)\n",
    "        question_sentence = re.sub(r\"\\[\\d+\\]\", \"\", question_sentence)\n",
    "        question_sentence = re.sub(r\"\\(\\w{1,3}\\)\", \"\", question_sentence)\n",
    "\n",
    "        # Get embedding for the question\n",
    "        question_embedding = self.model.encode(question_sentence, convert_to_numpy=True)\n",
    "\n",
    "        # Calculate cosine similarities\n",
    "        similarities = np.dot(self.corpus_embeddings, question_embedding) / (\n",
    "            np.linalg.norm(self.corpus_embeddings, axis=1)\n",
    "            * np.linalg.norm(question_embedding)\n",
    "        )\n",
    "\n",
    "        # Get top 10 most similar syllabus points\n",
    "        top_indices = np.argsort(similarities)[-10:][::-1]\n",
    "\n",
    "        # Count syllabus occurrences to find the most likely match\n",
    "        syllabus_votes = Counter()\n",
    "\n",
    "        for idx in top_indices:\n",
    "            syllabus_idx = self.syllabus_mapping[idx]\n",
    "            score = similarities[idx]\n",
    "            if score > threshold: \n",
    "                syllabus_votes[syllabus_idx] += score\n",
    "\n",
    "        # If we got any valid results, return the most common syllabus\n",
    "        if syllabus_votes:\n",
    "            best_syllabus_idx = syllabus_votes.most_common(1)[0][0]\n",
    "            return self.syllabus_objects[best_syllabus_idx]\n",
    "\n",
    "        # Fallback if no matches\n",
    "        return Syllabus(\"0\", \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e86c9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.6063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] \u001b[1;36m[1/4] Opening document...\u001b[0m\n",
      "[INFO] \u001b[1;36m[2/4] Analyzing document...\u001b[0m\n",
      "[WARNING] Ignore Line \"<image>\" due to overlap\n",
      "[INFO] \u001b[1;36m[3/4] Parsing pages...\u001b[0m\n",
      "[INFO] (1/15) Page 1\n",
      "[INFO] (2/15) Page 2\n",
      "[INFO] (3/15) Page 3\n",
      "[INFO] (4/15) Page 4\n",
      "[INFO] (5/15) Page 5\n",
      "[INFO] (6/15) Page 6\n",
      "[INFO] (7/15) Page 7\n",
      "[INFO] (8/15) Page 8\n",
      "[INFO] (9/15) Page 9\n",
      "[INFO] (10/15) Page 10\n",
      "[INFO] (11/15) Page 11\n",
      "[INFO] (12/15) Page 12\n",
      "[INFO] (13/15) Page 13\n",
      "[INFO] (14/15) Page 14\n",
      "[INFO] (15/15) Page 15\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(\"papers/595426-2023-2025-syllabus.pdf\") as syllabus_pdf:\n",
    "        syllabus_parser = SyllabusParser(syllabus_pdf, pages=(12, 46))\n",
    "        syllabuses = syllabus_parser.parse_syllabus()\n",
    "with pdfplumber.open(\"papers/igcse-biology-0610/0610_w22_qp_42.pdf\") as qppdf:\n",
    "    sq_parser = QuestionPaperParser(qppdf, image_prefix=\"0610_w22_qp_42\")\n",
    "    questions = sq_parser.parse_question_paper()\n",
    "sqms_parser = SQMSParser(\"papers/igcse-biology-0610/0610_w22_ms_42.pdf\", questions)\n",
    "questions = sqms_parser.parse_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4b3444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SentenceTransformer with model: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Use pytorch device_name: cuda:0\n",
      "[INFO] Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings from cache: biology_syllabus.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.27it/s]05,  1.18s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.00it/s]02,  1.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.48it/s]02,  1.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.44it/s]01,  1.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 45.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.79it/s]00,  1.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 42.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.60it/s]\n",
      "Classifying questions: 100%|██████████| 6/6 [00:03<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(syllabuses, cache_path=\"biology_syllabus.pt\")\n",
    "questions = classifier.classify_all(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f223bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question_hierarchy(questions):\n",
    "    output = \"\"\n",
    "    for q in questions:\n",
    "        output += f\"{q.text}\\n{q.syllabus.title if hasattr(q, \"syllabus\") else \"\"}\\n \"\n",
    "        if q.subquestions:\n",
    "            for sub_q in q.subquestions:\n",
    "                text = sub_q.text.strip()\n",
    "                output += f\"\\n    {text}\\n{sub_q.syllabus.title if hasattr(sub_q, \"syllabus\") else \"\"}\"\n",
    "                if sub_q.subsubquestions:\n",
    "                    for subsub_q in sub_q.subsubquestions:\n",
    "                        text = subsub_q.text.strip()\n",
    "                        subsub_q: SubSubQuestion\n",
    "                        output += f\"\\n        {text}\\n{subsub_q.syllabus.title if hasattr(subsub_q, \"syllabus\") else \"\"}\\n\"\n",
    "        output += \"\\n\" + \"-\" * 80 + \"\\n\"\n",
    "    return output.strip()\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(format_question_hierarchy(questions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
